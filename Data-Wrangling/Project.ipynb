{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from pymongo import cursor\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "import pprint\n",
    "import csv\n",
    "import json\n",
    "import io\n",
    "import re\n",
    "import codecs\n",
    "import pymongo\n",
    "\n",
    "client = MongoClient('localhost:27017')\n",
    "db = client.osmstreetmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Area Dataset - Boston, Massachusetts\n",
    "- boston_massachusetts.osm ... 211 MB\n",
    "- boston_massachusetts.json ... 245 MB\n",
    "\n",
    "#### Boston, MA\n",
    "Boston is the capital city and the most populous municipality of the Commonwealth of Massachusetts, USA. With population over 670,000 in 2016, it is the largest city in the New England region of the Northeastern United States. Founded in 1630 by Puritan settlers from England, it is considered as one of the oldest cities in the country. With universities like MIT, Harvard University, Berklee College of Music, University of Massachusetts etc., Boston is considered as an international center of higher education and also a world leader in innovation and entrepreneurship with nearly 2,000 start-ups.\n",
    "\n",
    "#### Why Boston?\n",
    "For a long time, I have wished to go to Boston, and I'm taking my first step towards knowing the place, by choosing this particular dataset to learn more about it before hand.\n",
    "\n",
    "#### Note\n",
    "The dataset can be found via this link: https://mapzen.com/data/metro-extracts/metro/boston_massachusetts/. The original dataset is over 400 MB and unfortunately my computer wasn't able to process the file of this magnitude, so I had to trim down the dataset to about 200 MB for it to function properly.\n",
    "\n",
    "### Skimming through it, I encountered few problems like:\n",
    "- How to better understand and clean up the dataset of such large size.\n",
    "- Several inconsistencies in zip codes, like the ones with more than five digits \"02110-1301\" or zip codes out of Boston area.\n",
    "- Abbreviation of some street types, like \"Ave\" for Avenue, \"St\" for Street etc.\n",
    "- The \"address\" section for some documents might differ from others, for instance, one way of representing the address in the documents is:\n",
    "        <tag k=\"address\" v=\"40 Thorndike St, Cambridge, MA, 02141\" />\n",
    "     having the entire address in one line. But some documents have separate entities describing an address like:\n",
    "        <tag k=\"addr:city\" v=\"Hyde Park\" />\n",
    "        <tag k=\"addr:street\" v=\"Metropolitan Avenue\" />\n",
    "        <tag k=\"addr:postcode\" v=\"02136\" />\n",
    "        <tag k=\"addr:housenumber\" v=\"655\" />\n",
    "    Having an address described the way above, makes it easy to go after cities, street or postcode separately. But with one-liner address, it might be difficult to get an accurate count, say most mentioned street or cities. So, I'm going to identify the zip code from the address part and save it separately like so, \"address\": {\"location\": \"40 Thorndike Street, Cambridge, MA 02141\", \"postcode\": \"02141\"}, to easily analyze all the zip codes in one go.\n",
    "\n",
    "\n",
    "### After the intial observation, my next steps would be to:\n",
    "- To count the tags, like the number of nodes, ways etc.\n",
    "- Identify any problem characters\n",
    "- Clean up the dataset, convert it to JSON and upload to MongoDB\n",
    "- Calculate the total number of documents\n",
    "- Number of unique contributors\n",
    "- Users with highest contributions\n",
    "- Users with 10 or less contributions\n",
    "- Drilling down on the zip codes, we'll examine if the zip codes are valid and whether they all belong to the Boston area\n",
    "- Most mentioned cities\n",
    "- List of amenities and their count\n",
    "\n",
    "### Ideas for improvement:\n",
    "- For addresses, there should be one standard way to store the data, like the following:\n",
    "        <tag k=\"type\" v=\"multipolygon\" />\n",
    "\t\t<tag k=\"building\" v=\"commercial\" />\n",
    "\t\t<tag k=\"addr:city\" v=\"Boston\" />\n",
    "\t\t<tag k=\"addr:state\" v=\"MA\" />\n",
    "\t\t<tag k=\"addr:street\" v=\"Summer Street\" />\n",
    "\t\t<tag k=\"building:height\" v=\"34.1376\" />\n",
    "\t\t<tag k=\"building:levels\" v=\"10\" />\n",
    "\t\t<tag k=\"addr:housenumber\" v=\"280\" />\n",
    "    This way of representing the address is quite convenient, not only it details out everything clearly but also it would be easy to examine each of the entities separately.\n",
    "- It would be a good addition to have the date or year of establishment for buildings, universities or libraries or places of interest or historical value.\n",
    "- Also, adding the capacity attribute to restaurants, buildings or necessary public places.\n",
    "\n",
    "Although it might be difficult for some to gather such minutiae details, and ofcourse to edit the already existing data, but these improvements might help people looking for specific data at times.\n",
    "\n",
    "### Conclusion:\n",
    "The dataset is quite well maintained for most part and it was fairly interesting & challenging to work with data of such large scale and examining the outcomes. There's always a room to improve the quality of content for hightened accessbility even by a novice.\n",
    "\n",
    "Before this lesson, I had little to no idea what OpenStreetMap was but these lessons along with this project have got me several steps closer to understanding the fundamentals of this phenomenal opensource collaborative mapping service. OpenStreetMap today has over 2 million registered users (Res: OpenStreetMap Wiki) and with more awareness, people all over the globe can have the knowledge to successfully build the map of the world.\n",
    "\n",
    "### Resources used:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/OpenStreetMap\n",
    "- https://api.mongodb.com/python/2.0/tutorial.html\n",
    "- https://docs.mongodb.com/manual/reference/method/\n",
    "- https://en.wikipedia.org/wiki/Boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OSM_FILE = \"boston_massachusetts.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"boston_massachusetts_sample.osm\"\n",
    "\n",
    "k = 100 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes = 976373\n",
      "Ways = 155519\n",
      "Relations = 663\n"
     ]
    }
   ],
   "source": [
    "# To count tags - Nodes, Ways, Relations\n",
    "def count_tags(filename):\n",
    "    tag_dictionary = {}\n",
    "    attrib_value_dictionary = {}\n",
    "    for event, element in ET.iterparse(filename, events = (\"start\",)):\n",
    "        key = element.tag\n",
    "        if key in tag_dictionary:\n",
    "            tag_dictionary[key] += 1\n",
    "        else:\n",
    "            tag_dictionary[key] = 1\n",
    "    \n",
    "    return tag_dictionary\n",
    "\n",
    "tags = count_tags(\"boston_massachusetts.osm\")\n",
    "print \"Nodes =\", tags['node']\n",
    "print \"Ways =\", tags['way']\n",
    "print \"Relations =\", tags['relation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "problemchars -- 3\n",
      "lower -- 398383\n",
      "other -- 19787\n",
      "lower_colon -- 37812\n"
     ]
    }
   ],
   "source": [
    "# Identifying problem characters\n",
    "\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        k_value = element.attrib['k']\n",
    "        if lower.search(k_value):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif lower_colon.search(k_value):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif problemchars.search(k_value):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else:\n",
    "            keys[\"other\"] += 1\n",
    "    return keys\n",
    "\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "    return keys\n",
    "\n",
    "def test():\n",
    "    keys = process_map('boston_massachusetts.osm')\n",
    "    for key, value in keys.iteritems():\n",
    "        print key, \"--\", value\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data cleanup and convert it to JSON\n",
    "CREATED = [\"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "mapping = {\"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\" : \"Avenue\",\n",
    "            \"Ave.\" : \"Avenue\",\n",
    "            \"Rd.\" : \"Road\",\n",
    "            \"Rd\" : \"Road\",\n",
    "            \"Blvd\" : \"Boulevard\",\n",
    "            \"Dr\" : \"Drive\",\n",
    "            \"pkwy\" : \"Parkway\",\n",
    "            \"Pkwy\" : \"Parkway\",\n",
    "            \"Trl\" : \"Trail\",\n",
    "            \"Ln\" : \"Lane\",\n",
    "            \"ct\" : \"Court\",\n",
    "            \"Ct\" : \"Court\"}\n",
    "\n",
    "def street_cleanup(street):\n",
    "    name_split = street.split(' ')\n",
    "    for key in mapping:\n",
    "        for word in name_split:\n",
    "            if key == word:\n",
    "                street = re.sub(r'\\b%s[.,\\b]?' %key, mapping[key], street)\n",
    "    return street\n",
    "\n",
    "def zip_code_setup(address):\n",
    "    zip_code_1 = re.compile(\"MA, \")\n",
    "    zip_code_2 = re.compile(\"MA \")\n",
    "    zip_pos = 10000\n",
    "    if zip_code_1.search(address):\n",
    "        zip_pos = zip_code_1.search(address).start()+4\n",
    "    if zip_code_2.search(address):\n",
    "        zip_pos = zip_code_2.search(address).start()+3\n",
    "    zip_code = address[zip_pos:].strip(' ')\n",
    "    return zip_code\n",
    "\n",
    "def capacity_conversion(value):\n",
    "    special_pattern = re.compile(\"~\")\n",
    "    if special_pattern.match(value):\n",
    "        return int(value[1:])\n",
    "    return int(value)\n",
    "\n",
    "def create_dictionary(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" or element.tag == \"relation\":\n",
    "        node['node_id'] = element.attrib['id']\n",
    "        node['node_type'] = element.tag\n",
    "        node['created'] = {}\n",
    "        \n",
    "        # The \"created\" attribute will store info on the 'id', 'date-time stamp', 'version' etc.\n",
    "        for attributes in CREATED:\n",
    "            if element.attrib[attributes]:\n",
    "                node['created'][attributes] = element.attrib[attributes]\n",
    "        \n",
    "        # We check if the current tag contains \"latitude\" or \"longitude\" values, if yes then we create\n",
    "        # their respective attributes\n",
    "        if 'lat' in element.attrib or 'lon' in element.attrib:\n",
    "            node['pos'] = []\n",
    "            latitude = element.attrib['lat']\n",
    "            longitude = element.attrib['lon']\n",
    "            node['pos'].append(latitude)\n",
    "            node['pos'].append(longitude)\n",
    "        \n",
    "        # The \"address\" attribute\n",
    "        node['address'] = {}\n",
    "        for tag in element.iter(\"tag\"):\n",
    "            \n",
    "            # We'll use the following, \"addr_pattern\" & \"colon_pattern\", regexes to filter out the \"tags\" that might\n",
    "            # contain any address related data\n",
    "            \n",
    "            # Addresses that match the \"addr:\" prefix, for e.g., <tag k=\"addr:postcode\" v=\"02126\" />\n",
    "            addr_pattern = re.compile(\"addr:\")\n",
    "            \n",
    "            # It will check if the address type begins with a colon, like <tag k=\":postcode\" v=\"02126\" /> and\n",
    "            # also to check for any additional colons in the address type\n",
    "            colon_pattern = re.compile(\":\")\n",
    "            \n",
    "            # To store the k-attribute\n",
    "            k_attrib = tag.attrib['k']\n",
    "            \n",
    "            # Stores the value for the k-attribute\n",
    "            value = tag.attrib['v']\n",
    "            \n",
    "            # This condition will perform any kind of clean up related to street name abbreviation based on the\n",
    "            # \"mapping\" dictionary provided, like converting \"St.\" to \"Street\" or \"Ave\" to \"Avenue\"\n",
    "            if k_attrib == \"addr:street\":\n",
    "                value = street_cleanup(value)\n",
    "                            \n",
    "            # If the regexes don't match with the attributes\n",
    "            if addr_pattern.match(k_attrib) == None and colon_pattern.match(k_attrib) == None:\n",
    "                \n",
    "                # The following condition is to check for the attributes stored as \"address\" instead of \"addr:\",\n",
    "                # like <tag k=\"address\" v=\"888 Broadway, Everett MA 02149-3199\" />, here we save the postcode as\n",
    "                # a separate entity\n",
    "                if k_attrib == \"address\":\n",
    "                    node['address']['location'] = value\n",
    "                    zip_code = zip_code_setup(value)\n",
    "                    if len(zip_code) >= 5:\n",
    "                        node['address']['postcode'] = zip_code\n",
    "\n",
    "                else:\n",
    "                    # To convert the value for \"capacity\" attribute from String type to Integer\n",
    "                    if k_attrib == \"capacity\":\n",
    "                        node[k_attrib] = capacity_conversion(value)\n",
    "                    else:\n",
    "                        node[k_attrib] = value\n",
    "            \n",
    "            # If the regexes do match, then the matched prefixes will be removed saving just the address type with their\n",
    "            # corresponding values\n",
    "            else:\n",
    "                if addr_pattern.match(k_attrib):\n",
    "                    addr_type = k_attrib[5:]\n",
    "                elif colon_pattern.match(k_attrib):\n",
    "                    addr_type = k_attrib[1:]\n",
    "                if not colon_pattern.search(addr_type):\n",
    "                    node['address'][addr_type] = value\n",
    "        \n",
    "        # If no addresses are found for a particular tag, then it will be dropped from the dictionary\n",
    "        if node['address'] == {}:\n",
    "            node.pop('address', None)\n",
    "        \n",
    "        # If the tag is \"way\", then all of it's node references, like <nd ref=\"240167956\" />, will be saved as a list\n",
    "        if element.tag == \"way\":\n",
    "            node['node_refs'] = []\n",
    "            for tag in element.iter(\"nd\"):\n",
    "                node_refs = tag.attrib['ref']\n",
    "                node['node_refs'].append(node_refs)\n",
    "        \n",
    "        # Same with \"relation\", all it's members, like <member ref=\"244444287\" role=\"across\" type=\"way\" /> will be saved\n",
    "        # as a dictionary\n",
    "        if element.tag == \"relation\":\n",
    "            node['member_values'] = {}\n",
    "            for tag in element.iter(\"member\"):\n",
    "                node['member_values']['ref'] = tag.attrib['ref']\n",
    "                node['member_values']['role'] = tag.attrib['role']\n",
    "                node['member_values']['type'] = tag.attrib['type']\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_map(file_in):\n",
    "    file_out = \"boston_massachusetts.json\".format(file_in)\n",
    "    with codecs.open(file_out, \"w\") as fo:\n",
    "        for _, element in ET.iterparse(file_in):\n",
    "            el = create_dictionary(element)\n",
    "            if el:\n",
    "                fo.write(json.dumps(el) + \"\\n\")\n",
    "\n",
    "process_map('boston_massachusetts.osm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of documents: 1132555\n"
     ]
    }
   ],
   "source": [
    "# Total no. of documents\n",
    "total = db.boston_massachusetts.find().count()\n",
    "print \"Total no. of documents:\", total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of unique contributors: 1197\n"
     ]
    }
   ],
   "source": [
    "# No. of unique contributors\n",
    "unique_contributors = len(db.boston_massachusetts.distinct(\"created.uid\"))\n",
    "print \"No. of unique contributors:\", unique_contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCS SUBMITTED, (contribution %ge) -->  USERS\n",
      "----------------------------------      -----\n",
      "597677 , (52.77)                   -->    crschmidt\n",
      "214329 , (18.92)                   -->    jremillard-massgis\n",
      "55587 , (4.91)                   -->    wambag\n",
      "45447 , (4.01)                   -->    OceanVortex\n",
      "33690 , (2.97)                   -->    morganwahl\n",
      "32991 , (2.91)                   -->    ryebread\n",
      "29291 , (2.59)                   -->    MassGIS Import\n",
      "16220 , (1.43)                   -->    ingalls_imports\n",
      "14125 , (1.25)                   -->    Ahlzen\n",
      "7363 , (0.65)                   -->    mapper999\n",
      "\n",
      "TOTAL DOCS SUBMITTED BY THE TOP 10 USERS, (contribution %ge)\n",
      "---------------------------------------------------------------\n",
      "1046720 , (92.42)\n"
     ]
    }
   ],
   "source": [
    "# Users with highest contributions & percentage\n",
    "max_contributors = db.boston_massachusetts.aggregate([{\"$group\" : {\"_id\" : \"$created.user\",\n",
    "                                                               \"contributions\" : {\"$sum\" : 1}}},\n",
    "                                                      {\"$sort\" : {\"contributions\" : -1}},\n",
    "                                                      {\"$limit\" : 10}])\n",
    "\n",
    "print \"DOCS SUBMITTED, (contribution %ge) -->  USERS\"\n",
    "print \"----------------------------------      -----\"\n",
    "top_ten_docs = 0\n",
    "for contributor in max_contributors:\n",
    "    top_ten_docs += contributor['contributions']\n",
    "    print contributor['contributions'], \", (%.2f)\" % float(contributor['contributions'] * 100.0/total), '                  -->   ',contributor['_id']\n",
    "\n",
    "print\n",
    "print \"TOTAL DOCS SUBMITTED BY THE TOP 10 USERS, (contribution %ge)\"\n",
    "print \"---------------------------------------------------------------\"\n",
    "print top_ten_docs, \", (%.2f)\" % float(top_ten_docs * 100.0/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCUMENTS --> NO. OF USERS, (%ge of users)\n",
      "---------     ----------------------------\n",
      "1         -->         338 , (28.24)\n",
      "2         -->         128 , (10.69)\n",
      "3         -->         88 , (7.35)\n",
      "4         -->         60 , (5.01)\n",
      "5         -->         50 , (4.18)\n",
      "6         -->         36 , (3.01)\n",
      "7         -->         24 , (2.01)\n",
      "8         -->         23 , (1.92)\n",
      "9         -->         20 , (1.67)\n",
      "10         -->         15 , (1.25)\n",
      "\n",
      "TOTAL, (%ge of users with 10 docs or less)\n",
      "------------------------------------------\n",
      "782 , (65.33)\n"
     ]
    }
   ],
   "source": [
    "# No. of users with 10 or less contributions\n",
    "min_contributors = db.boston_massachusetts.aggregate([{\"$group\" : {\"_id\" : \"$created.user\",\n",
    "                                                                   \"contributions\" : {\"$sum\" : 1}}},\n",
    "                                                      {\"$group\" : {\"_id\" : \"$contributions\",\n",
    "                                                                   \"users_count\" : {\"$sum\" : 1}}},\n",
    "                                                      {\"$sort\" : {\"_id\" : 1}},\n",
    "                                                      {\"$limit\" : 10}])\n",
    "print \"DOCUMENTS --> NO. OF USERS, (%ge of users)\"\n",
    "print \"---------     ----------------------------\"\n",
    "bottom_ten_users = 0\n",
    "for contributor in min_contributors:\n",
    "    bottom_ten_users += contributor['users_count']\n",
    "    print contributor['_id'], '        -->        ', contributor['users_count'], \", (%.2f)\" % float(contributor['users_count'] * 100.0/unique_contributors)\n",
    "\n",
    "print\n",
    "print \"TOTAL, (%ge of users with 10 docs or less)\"\n",
    "print \"------------------------------------------\"\n",
    "print bottom_ten_users, \", (%.2f)\" % float(bottom_ten_users * 100.0/unique_contributors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents with \"postcode\" in the \"address\" section: 1593\n",
      "Documents with \"postcode\" in the Boston area: 833\n",
      "Documents with \"postcode\" outside the Boston area: 760\n",
      "\n",
      "Unique postcodes in the documents: 75\n",
      "All of the Boston area postcodes: 62\n",
      "Postcodes from the documents belonging to the Boston area: 40\n",
      "Postcodes not belonging to the Boston area: 35\n",
      "\n",
      "10 most mentioned zip codes:\n",
      "----------------------------\n",
      "02139  -->  218\n",
      "02135  -->  161\n",
      "02130  -->  105\n",
      "02144  -->  63\n",
      "02474  -->  63\n",
      "02114  -->  49\n",
      "02215  -->  46\n",
      "02116  -->  43\n",
      "02143  -->  40\n",
      "02138  -->  39\n"
     ]
    }
   ],
   "source": [
    "# Zip codes\n",
    "\n",
    "# All the zip codes in the Boston area\n",
    "boston_zip_codes = ['01841', '02101', '02108', '02109', '02110', '02111', '02112', '02113', '02114', '02115', '02116', '02117',\n",
    "                    '02118', '02119', '02120', '02121', '02122', '02123', '02124', '02125', '02126', '02127', '02128', '02129',\n",
    "                    '02130', '02131', '02132', '02133', '02134', '02135', '02136', '02137', '02141', '02149', '02150', '02151',\n",
    "                    '02152', '02163', '02171', '02196', '02199', '02201', '02203', '02204', '02205', '02206', '02210', '02211',\n",
    "                    '02212', '02215', '02217', '02222', '02228', '02241', '02266', '02283', '02284', '02293', '02297', '02298',\n",
    "                    '02445', '02467']\n",
    "\n",
    "# Documents with \"postcode\" in the \"address\" section\n",
    "docs_with_postcode = db.boston_massachusetts.find({'address.postcode' : {\"$exists\" : True}}).count()\n",
    "print \"Documents with \\\"postcode\\\" in the \\\"address\\\" section:\", docs_with_postcode\n",
    "\n",
    "# Documents with \"postcode\" in Boston area\n",
    "docs_with_postcode_in_boston = db.boston_massachusetts.find({'address' : {\"$exists\" : True},\n",
    "                                                             'address.postcode' : {\"$in\" : boston_zip_codes}}).count()\n",
    "print \"Documents with \\\"postcode\\\" in the Boston area:\", docs_with_postcode_in_boston\n",
    "\n",
    "# Documents with \"postcode\" not in the Boston area\n",
    "docs_with_postcode_outside = db.boston_massachusetts.find({'address' : {\"$exists\" : True},\n",
    "                                                           'address.postcode' : {\"$exists\" : True,\n",
    "                                                                                 \"$nin\" : boston_zip_codes}}).count()\n",
    "print \"Documents with \\\"postcode\\\" outside the Boston area:\", docs_with_postcode_outside\n",
    "\n",
    "docs_with_postcode_outside_details = db.boston_massachusetts.find({'address' : {\"$exists\" : True},\n",
    "                                                                   'address.postcode' : {\"$exists\" : True,\n",
    "                                                                                         \"$nin\" : boston_zip_codes}})\n",
    "\n",
    "# On further expansion of the \"docs_with_postcode_outside\" the Boston area, we can see some that of the postcodes have been\n",
    "# formatted differently, for e.g. \"02110-1301\", and thus needs to be edited to get an accurate count.\n",
    "set_code = set()\n",
    "for index, area in zip(range(200), docs_with_postcode_outside_details):\n",
    "    address = area['address']\n",
    "    if 'postcode' in address:\n",
    "        set_code.add(address['postcode'])\n",
    "\n",
    "# You can uncomment the below print statement to view a sample of such zip codes\n",
    "# pprint.pprint(set_code)\n",
    "\n",
    "# Edit the zip codes to 5-digit values by removing any hypens or any preceding characters like \"MA\", to get a count on unique zip codes from the documents\n",
    "set_postcodes = set()\n",
    "for area in db.boston_massachusetts.find({\"address.postcode\" : {\"$exists\" : True}}):\n",
    "    postcode = area['address']['postcode']\n",
    "    type_1 = re.compile(\"-\")\n",
    "    type_2 = re.compile(\"MA \")\n",
    "    if type_1.search(postcode):\n",
    "        pos = type_1.search(postcode).start()\n",
    "        postcode = postcode[:pos]\n",
    "    if type_2.search(postcode):\n",
    "        postcode = postcode[3:]\n",
    "    set_postcodes.add(postcode)\n",
    "\n",
    "print\n",
    "print \"Unique postcodes in the documents:\", len(set_postcodes)\n",
    "print \"All of the Boston area postcodes:\", len(boston_zip_codes)\n",
    "print \"Postcodes from the documents belonging to the Boston area:\", len(set_postcodes.intersection(boston_zip_codes))\n",
    "print \"Postcodes not belonging to the Boston area:\", len((set_postcodes) - set(boston_zip_codes))\n",
    "print\n",
    "\n",
    "# Most mentioned postal codes in the documents\n",
    "most_mentioned_postal_codes = db.boston_massachusetts.aggregate([{\"$match\" : {\"address.postcode\" : {\"$exists\" : True}}},\n",
    "                                                                 {\"$group\" : {\"_id\" : \"$address.postcode\",\n",
    "                                                                              \"count\" : {\"$sum\" : 1}}},\n",
    "                                                                 {\"$sort\" : {\"count\" : -1}},\n",
    "                                                                 {\"$limit\" : 10}])\n",
    "\n",
    "print \"10 most mentioned zip codes:\"\n",
    "print \"----------------------------\"\n",
    "for data in most_mentioned_postal_codes:\n",
    "    print data['_id'], \" --> \", data['count']\n",
    "\n",
    "# The most mentioned postal code - 02139, doesn't come under Boston but is actually in Cambridge, MA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Most mentioned cities\n",
      "------------------------\n",
      "Boston  -->  572\n",
      "Cambridge  -->  287\n",
      "Malden  -->  191\n",
      "Arlington  -->  136\n",
      "Somerville  -->  134\n",
      "Jamaica Plain  -->  50\n",
      "Chelsea  -->  37\n",
      "Quincy  -->  28\n",
      "Medford  -->  25\n",
      "Brookline  -->  22\n"
     ]
    }
   ],
   "source": [
    "# Most mentioned cities\n",
    "most_mentioned_city = db.boston_massachusetts.aggregate([{\"$match\" : {\"address.city\" : {\"$exists\" : True}}},\n",
    "                                                         {\"$group\" : {\"_id\" : \"$address.city\",\n",
    "                                                                      \"count\" : {\"$sum\" : 1}}},\n",
    "                                                         {\"$sort\" : {\"count\" : -1}},\n",
    "                                                         {\"$limit\" : 10}])\n",
    "\n",
    "print \"10 Most mentioned cities\"\n",
    "print \"------------------------\"\n",
    "for data in most_mentioned_city:\n",
    "    print data['_id'], \" --> \", data['count']\n",
    "\n",
    "# Mostly true to the dataset, \"Boston\" has the highest mentions in the documents but as we can see we have names of surrounding\n",
    "# cities as well like \"Cambridge\", \"Malden\" etc. which, explains why we have about 35 of zip codes outside of Boston area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIST OF AMENITIES AND COUNT:\n",
      "----------------------------\n",
      "parking  -->  742\n",
      "bench  -->  527\n",
      "restaurant  -->  376\n",
      "school  -->  339\n",
      "parking_space  -->  224\n",
      "place_of_worship  -->  212\n",
      "library  -->  157\n",
      "bicycle_parking  -->  155\n",
      "cafe  -->  151\n",
      "fast_food  -->  127\n",
      "\n",
      "PLACES OF WORSHIP:\n",
      "------------------\n",
      "christian  -->  183\n",
      "jewish  -->  9\n",
      "unitarian_universalist  -->  4\n",
      "muslim  -->  2\n",
      "buddhist  -->  1\n",
      "\n",
      "CUISINES\n",
      "--------\n",
      "pizza  -->  32\n",
      "mexican  -->  20\n",
      "chinese  -->  17\n",
      "indian  -->  15\n",
      "italian  -->  15\n",
      "\n",
      "RESTAURANTS WITH HIGHEST CAPACITY\n",
      "---------------------------------\n",
      "The Haven --> 100\n",
      "Chicago Pizza --> 40\n",
      "Yume Wo Katare --> 20\n",
      "Taco Party --> 20\n",
      "All Star Pizza Bar --> 20\n",
      "\n",
      "Restaurants with no capacity info: 367\n"
     ]
    }
   ],
   "source": [
    "# List of amenities and their count\n",
    "amenities = db.boston_massachusetts.aggregate([{\"$match\" : {\"amenity\" : {\"$exists\" : True}}},\n",
    "                                               {\"$group\" : {\"_id\" : \"$amenity\",\n",
    "                                                            \"count\" : {\"$sum\" : 1}}},\n",
    "                                               {\"$sort\" : {\"count\" : -1}},\n",
    "                                               {\"$limit\" : 10}])\n",
    "\n",
    "print\n",
    "print \"LIST OF AMENITIES AND COUNT:\"\n",
    "print \"----------------------------\"\n",
    "for data in amenities:\n",
    "    print data['_id'], \" --> \", data['count']\n",
    "print\n",
    "\n",
    "# Places of worship\n",
    "places_of_worship = db.boston_massachusetts.aggregate([{\"$match\" : {\"amenity\" : {\"$exists\" : True}, \"amenity\" : \"place_of_worship\", \"religion\" : {\"$exists\" : True}}},\n",
    "                                                       {\"$group\" : {\"_id\" : \"$religion\",\n",
    "                                                                    \"count\" : {\"$sum\" : 1}}},\n",
    "                                                       {\"$sort\" : {\"count\" : -1}}])\n",
    "\n",
    "print \"TOP 5 PLACES OF WORSHIP:\"\n",
    "print \"------------------------\"\n",
    "for data in places_of_worship:\n",
    "    print data['_id'], \" --> \", data['count']\n",
    "print\n",
    "\n",
    "# Restaurants & cuisine\n",
    "restaurants = db.boston_massachusetts.aggregate([{\"$match\" : {\"amenity\" : {\"$exists\" : True}, \"amenity\" : \"restaurant\", \"cuisine\" : {\"$exists\" : True}}},\n",
    "                                                 {\"$group\" : {\"_id\" : \"$cuisine\",\n",
    "                                                              \"count\" : {\"$sum\" : 1}}},\n",
    "                                                 {\"$sort\" : {\"count\" : -1}},\n",
    "                                                 {\"$limit\" : 5}])\n",
    "\n",
    "print \"TOP 5 CUISINES\"\n",
    "print \"--------------\"\n",
    "for data in restaurants:\n",
    "    print data['_id'], \" --> \", data['count']\n",
    "\n",
    "# Top 5 restaurants with highest capacity\n",
    "top_capacity_restaurants = db.boston_massachusetts.find({\"amenity\" : \"restaurant\", \"capacity\" : {\"$exists\" : True}}).sort(\"capacity\", -1).limit(5)\n",
    "\n",
    "print\n",
    "print \"TOP 5 RESTAURANTS WITH HIGHEST CAPACITY\"\n",
    "print \"---------------------------------------\"\n",
    "for data in top_capacity_restaurants:\n",
    "    print data['name'], \"-->\", data['capacity']\n",
    "\n",
    "print\n",
    "res_with_no_capacity_info = db.boston_massachusetts.find({\"amenity\" : {\"$exists\" : True}, \"amenity\" : \"restaurant\", \"capacity\" : {\"$exists\" : False}}).count()\n",
    "print \"Restaurants with no capacity info:\", res_with_no_capacity_info\n",
    "    \n",
    "# As you can see, there are still over 300 restaurants with no capacity data. Having the this info for all the restaurants would have given us a better idea"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
